{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "injured-ranch",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "This step is taken care of in the accompanying preprocessing notebook. That notebook includes the details for each dataset and our cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "id": "changing-amber",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "endangered-outside",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raccoon-17.jpg</td>\n",
       "      <td>259</td>\n",
       "      <td>194</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>167</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  width  height    class  xmin  ymin  xmax  ymax\n",
       "0  raccoon-17.jpg    259     194  raccoon    95    60   167   118"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in the data \n",
    "data_path = '/Users/michellejanneycoyle/Desktop/Object_Detection_DL/images_rs'\n",
    "df = pd.read_csv('/Users/michellejanneycoyle/Desktop/DL_Assignments/Object_Detection_DL/Data/train_labels_.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-satisfaction",
   "metadata": {
    "tags": []
   },
   "source": [
    "See resized image below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "id": "combined-mexico",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rescale bounding box in preparation for rescaling images\n",
    "image_size = 128\n",
    "df['xmin_rs'] = df['xmin']*image_size/df['width']\n",
    "df['xmax_rs'] = df['xmax']*image_size/df['width']\n",
    "df['ymin_rs'] = df['ymin']*image_size/df['height']\n",
    "df['ymax_rs'] = df['ymax']*image_size/df['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "related-producer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images with >1 raccoon, remove to simplify model\n",
    "new = pd.DataFrame(df.filename.value_counts() > 1)\n",
    "new = new.reset_index()\n",
    "new = new[new['filename'] == True]\n",
    "for_removal = list(new['index'].values)\n",
    "len(for_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "confirmed-harvest",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 1132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images with 1 raccoon only\n",
    "df = df[~df['filename'].isin(for_removal)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-merit",
   "metadata": {},
   "source": [
    "### Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "id": "national-blackberry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-latvia",
   "metadata": {},
   "source": [
    "# PART ONE: Object Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-municipality",
   "metadata": {},
   "source": [
    "# Step 1: Create a PyTorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "invalid-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaccoonDataset(Dataset):\n",
    "    def __init__(self, df, root, augment=True):\n",
    "        # Dataframe with bounding boxes\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) # this only works if 1:1 image:label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # gathering image information from datafiels \n",
    "        row = self.df.iloc[idx]\n",
    "        fname = row['filename']\n",
    "        img_path = root + '/' + fname\n",
    "        img = cv2.imread(img_path)\n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # move color channels to correct spot\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        \n",
    "        # convert to [0,1] scale\n",
    "        img = torch.tensor(img / 255.).float()        \n",
    "        \n",
    "        # Get bounding box coordinates ()\n",
    "        xmin, ymin = tuple(list(row[['xmin_rs','ymin_rs']].values))\n",
    "        xmax, ymax = tuple(list(row[['xmax_rs','ymax_rs']].values))\n",
    "        \n",
    "        # \"normalizing\" the bounding boxes to the same size as the image\n",
    "        bbox = [xmin/128,ymin/128,xmax/128,ymax/128]\n",
    "        \n",
    "        # passing image and bounding box values through the albumentations pipeline \n",
    "        bbox = torch.tensor(bbox)\n",
    "        \n",
    "        return img, bbox\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "id": "necessary-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/michellejanneycoyle/Desktop/Object_Detection_DL/images_rs'\n",
    "train_ds = RaccoonDataset(train, root)\n",
    "valid_ds = RaccoonDataset(valid, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-milan",
   "metadata": {},
   "source": [
    "### Passing Data through a dataloader \n",
    "This will split your data into batches to pass through the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "id": "adjacent-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-investigator",
   "metadata": {},
   "source": [
    "### Function for running the model\n",
    "\n",
    "Below is the framework for running the model that we will employ throughout this notebook (with slight changes). Here you can see the loss function is defined as MSELoss. We are running the model for **25 Epochs**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "id": "linear-petroleum",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(model):\n",
    "    loss_fun = nn.MSELoss()\n",
    "    for epoch in range(25):\n",
    "        # Train\n",
    "        train_loss = 0\n",
    "        for x, y in train_dl:\n",
    "            model.train()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fun(y.float(), y_pred.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            \n",
    "        # Valid\n",
    "        val_loss = 0\n",
    "        for x, y in valid_dl:\n",
    "            model.eval()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fun(y.float(), y_pred.float())\n",
    "            val_loss += loss.item() * x.shape[0]\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch} - Train loss: {round(train_loss/len(train),4)}  Valid loss: {round(val_loss/len(valid),4)}')    \n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-garage",
   "metadata": {},
   "source": [
    "# Step 2: Building a Basic Model \n",
    "\n",
    "For this exploration we choose to experiment with a basic **Convolutional Neural Network (CNN)**. Our model has four convolutional layers with pooling between. We also use relu as our activation function and two linear layers at the end. Our final linear layer outputs a tensor with 4 predictions representing our bounding box coordinates.\n",
    "\n",
    "For our first round of testing we use **MSE loss**. In the next section we will use Intersection over Union loss and compare the outputs of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "id": "interested-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # same padding!\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 100)\n",
    "        self.linear2 = nn.Linear(100, 4)\n",
    "        \n",
    "        # pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # unroll x for FC layer\n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-course",
   "metadata": {},
   "source": [
    "As you can see below the basic model run with a basic dataset and simple CNN does not seem to train correctly. Neither the train loss or validation loss move. This could be cause by a number of factors including exploding/diminishing gradients. \n",
    "\n",
    "In the following sections we attempt two techniques **Data Augmentation** and **Batch Normalization** to try and help the basic CNN train better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "id": "confidential-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train loss: 0.3338  Valid loss: 0.3562\n",
      "\n",
      "Epoch 5 - Train loss: 0.3338  Valid loss: 0.3562\n",
      "\n",
      "Epoch 10 - Train loss: 0.3338  Valid loss: 0.3562\n",
      "\n",
      "Epoch 15 - Train loss: 0.3338  Valid loss: 0.3562\n",
      "\n",
      "Epoch 20 - Train loss: 0.3338  Valid loss: 0.3562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "model = CNN1()\n",
    "run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-tooth",
   "metadata": {},
   "source": [
    "# Step 2: PyTorch Dataset with Data Augmentation \n",
    "\n",
    "Below you will see that our dataset includes the basic operations of a dataset, but also randomly augments some of the images passed through it. For data augmentation we decided to use the **albumentations library**. This library has a number of helpful features that allow you to easily augment images in a pipeline structure. In our dataset we choose to randomly **vertically flip** and **horizontally flip** our images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "id": "early-rebel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RaccoonDatasetAug(Dataset):\n",
    "    def __init__(self, df, root, augment=True):\n",
    "        # Dataframe with bounding boxes\n",
    "        self.df = df\n",
    "        self.root = root\n",
    "        \n",
    "        # define the transformation\n",
    "        # this transformation pipeline will randomly apply vertical flips, horizontal flips\n",
    "        # to images and to bounding boxes \n",
    "        if augment == True:\n",
    "            self.transforms = A.Compose([\n",
    "                A.augmentations.transforms.VerticalFlip(p=.5),\n",
    "                A.augmentations.transforms.HorizontalFlip(p=.5),\n",
    "                ToTensorV2()], bbox_params = A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) \n",
    "        else:\n",
    "            self.transforms = A.Compose([\n",
    "                ToTensorV2()], bbox_params = A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) # this only works if 1:1 image:label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # gathering image information from datafiels \n",
    "        row = self.df.iloc[idx]\n",
    "        fname = row['filename']\n",
    "        img_path = root + '/' + fname\n",
    "        img = cv2.imread(img_path)\n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get bounding box coordinates ()\n",
    "        xmin, ymin = tuple(list(row[['xmin_rs','ymin_rs']].values))\n",
    "        xmax, ymax = tuple(list(row[['xmax_rs','ymax_rs']].values))\n",
    "        \n",
    "        # \"normalizing\" the bounding boxes to the same size as the image\n",
    "        bbox = [[xmin/128,ymin/128,xmax/128,ymax/128]]\n",
    "        class_labels = [row['class']]\n",
    "        \n",
    "        # passing image and bounding box values through the albumentations pipeline \n",
    "        transformed = self.transforms(image=img.astype(np.uint8),bboxes=bbox, class_labels=class_labels)\n",
    "        img = transformed['image'].float()\n",
    "        bbox = torch.tensor(transformed['bboxes'][0])\n",
    "        class_labels = transformed['class_labels']\n",
    "        \n",
    "        return img, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "id": "challenging-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/michellejanneycoyle/Desktop/Object_Detection_DL/images_rs'\n",
    "train_ds = RaccoonDatasetAug(train, root)\n",
    "valid_ds = RaccoonDatasetAug(valid, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "seeing-arthur",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "crazy-james",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train loss: 8141.8769  Valid loss: 7699.904\n",
      "\n",
      "Epoch 5 - Train loss: 8728.3686  Valid loss: 7918.4004\n",
      "\n",
      "Epoch 10 - Train loss: 8142.3972  Valid loss: 8137.4688\n",
      "\n",
      "Epoch 15 - Train loss: 7922.5999  Valid loss: 8358.6166\n",
      "\n",
      "Epoch 20 - Train loss: 8215.2542  Valid loss: 7918.9633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "model = CNN1()\n",
    "run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-senior",
   "metadata": {},
   "source": [
    "# Step 3: Adding Batch Normalization\n",
    "\n",
    "We noticed that the model above seems to have some difficulty learning the bounding boxes (i.e. the train and validation loss remain high and seem to jump around even with a very small learning rate). Therefore, we tried **batch normalization**. Batch Normalization is a common normalization technique used to prevent overfitting/learning unnecessary or incorrect patterns in the data. One important thing to keep in mind when using batch normalization is to use the **BatchNorm2d** function - this will handle image inputs that BatchNorm1d cannot handle. \n",
    "\n",
    "As you can see below, it looks like batch normalization did not work in this case. Unfortunately, we were unable to discover the cause of this issue. As with many things in DL this project is full of trial and error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "structural-moisture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # same padding!\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # doing this to shrink size enough!\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 100)\n",
    "        \n",
    "        # read documentation for CrossEntropy Loss!\n",
    "        self.linear2 = nn.Linear(100, 4)\n",
    "        \n",
    "        # pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # for unrolling into FC layer\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "        # using 2d because we are using images! input should be the size of out channels \n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # unroll x for FC layer\n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "sought-discharge",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train loss: 7854.0781  Valid loss: 7315.1471\n",
      "\n",
      "Epoch 5 - Train loss: 8128.8987  Valid loss: 6230.2537\n",
      "\n",
      "Epoch 10 - Train loss: 7302.6576  Valid loss: 7313.7482\n",
      "\n",
      "Epoch 15 - Train loss: 7786.9428  Valid loss: 8127.1346\n",
      "\n",
      "Epoch 20 - Train loss: 9506.9601  Valid loss: 7857.0988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "model = CNN2()\n",
    "run_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-bowling",
   "metadata": {},
   "source": [
    "# PART TWO: IOU as Loss Function\n",
    "\n",
    "**Intersection over Union (IoU)** is a helpful loss function and metric that can be used when training and evaluating the performance of an object detection model. The basic idea is that IoU measures the area shared between the ground truth bounding box and the predicted bounding box. If you look below you will see our adapted IoU function (citation in comment). \n",
    "\n",
    "Unfortunately, we were unable to get this IoU function to work properly with this data. For some reason we cannot get the function to produce a loss other than zero. \n",
    "\n",
    "In later iterations of this repository we will correct this function and compare how the model trains when using MSE vs IoU as a loss metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://github.com/t-vi/pytorch-tvmisc/blob/master/misc/pytorch_automatic_optimization_jit.ipynb\n",
    "def ratio_iou(y_pred_bbox, bbox):\n",
    "    # bbox comes in order of x1, y1, x2, y2\n",
    "    iou = 0\n",
    "    for i in range(bbox.shape[0]):\n",
    "        x1 = y_pred_bbox[i][0] # top left x coordinate, x1\n",
    "        y1 = y_pred_bbox[i][3] # top left y coordinate, y2 \n",
    "        w1 = torch.abs(y_pred_bbox[i][2] - y_pred_bbox[i])[0] # x2 - x1\n",
    "        h1 = torch.abs(y_pred_bbox[i][3] - y_pred_bbox[i])[1] # y2 - y1\n",
    "        x2 = bbox[i][0]\n",
    "        y2 = bbox[i][3]\n",
    "        w2 = torch.abs(bbox[i][2] - bbox[i][0])\n",
    "        h2 = torch.abs(bbox[i][3] - bbox[i][1])\n",
    "        xi = torch.max(x1, x2)                                 # Intersection\n",
    "        yi = torch.max(y1, y2)\n",
    "        wi = torch.clamp(torch.min(x1+w1, x2+w2) - xi, min=0)\n",
    "        wi = torch.min(x1+w1, x2+w2)\n",
    "        hi = torch.clamp(torch.min(y1+h1, y2+h2) - yi, min=0)\n",
    "        print(torch.min(x1+w1, x2+w2) - xi)\n",
    "        print(torch.clamp(torch.min(x1+w1, x2+w2) - xi, min=0))\n",
    "        area_i = wi * hi                                       # Area Intersection \n",
    "        area_u = w1 * h1 + w2 * h2 - wi * hi                   # Area Union\n",
    "#         print(area_i)\n",
    "        iou += (area_i / torch.clamp(area_u, min=1e-5))\n",
    "        return iou/bbox.shape[0]\n",
    "\n",
    "def ratio_iou_loss(y_pred_bbox, bbox):\n",
    "    return - ratio_iou(y_pred_bbox, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "juvenile-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN1()\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "animated-apache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 4]), torch.Size([10, 4]))"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "celtic-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-125.8052, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "tensor(0., dtype=torch.float64, grad_fn=<ClampBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0., dtype=torch.float64, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ratio_iou_loss(y, y_pred)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-guinea",
   "metadata": {},
   "source": [
    "# PART THREE: Classification and Object Detection\n",
    "\n",
    "In this repository we also want to provide the ground work for creating a **multi-task** model. Namely a model that can predict if a raccoon is present or not as well at locating the raccoon if one is detected. In order to create this model we combined the original raccoon dataset with a dataset that includes images of other animals. Details on the second dataset and preprocessing can be found in the preprocessing notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "id": "yellow-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in cleaned, and combined dataset (see preprocessing notebook) \n",
    "train_df = pd.read_csv(\"combined_train.csv\")\n",
    "valid_df = pd.read_csv(\"combined_valid.csv\")\n",
    "valid_df = valid_df.reset_index()\n",
    "train_df = train_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "after-cattle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>xmin_rs</th>\n",
       "      <th>xmax_rs</th>\n",
       "      <th>ymin_rs</th>\n",
       "      <th>ymax_rs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>raccoon-17.jpg</td>\n",
       "      <td>259.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>95.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>46.949807</td>\n",
       "      <td>82.532819</td>\n",
       "      <td>39.587629</td>\n",
       "      <td>77.855670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>raccoon-11.jpg</td>\n",
       "      <td>660.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>431.0</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>89.406061</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>127.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>raccoon-60.jpg</td>\n",
       "      <td>273.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>58.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>27.194139</td>\n",
       "      <td>92.366300</td>\n",
       "      <td>22.832432</td>\n",
       "      <td>87.870270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>raccoon-200.jpg</td>\n",
       "      <td>261.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>107.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>52.475096</td>\n",
       "      <td>122.114943</td>\n",
       "      <td>6.632124</td>\n",
       "      <td>110.093264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>raccoon-141.jpg</td>\n",
       "      <td>249.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.514056</td>\n",
       "      <td>79.164659</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>111.524752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>flickr_wild_002748.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>232</td>\n",
       "      <td>pixabay_wild_000700.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>233</td>\n",
       "      <td>233</td>\n",
       "      <td>flickr_wild_000289.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>pixabay_wild_000024.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>flickr_wild_000914.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0                 filename  width  height    class  \\\n",
       "0        0           0           raccoon-17.jpg  259.0   194.0  raccoon   \n",
       "1        1           1           raccoon-11.jpg  660.0   432.0  raccoon   \n",
       "2        2           2           raccoon-60.jpg  273.0   185.0  raccoon   \n",
       "3        3           3          raccoon-200.jpg  261.0   193.0  raccoon   \n",
       "4        4           4          raccoon-141.jpg  249.0   202.0  raccoon   \n",
       "..     ...         ...                      ...    ...     ...      ...   \n",
       "231    231         231   flickr_wild_002748.jpg    NaN     NaN     wild   \n",
       "232    232         232  pixabay_wild_000700.jpg    NaN     NaN     wild   \n",
       "233    233         233   flickr_wild_000289.jpg    NaN     NaN     wild   \n",
       "234    234         234  pixabay_wild_000024.jpg    NaN     NaN     wild   \n",
       "235    235         235   flickr_wild_000914.jpg    NaN     NaN     wild   \n",
       "\n",
       "      xmin  ymin   xmax   ymax    xmin_rs     xmax_rs    ymin_rs     ymax_rs  \n",
       "0     95.0  60.0  167.0  118.0  46.949807   82.532819  39.587629   77.855670  \n",
       "1      3.0   1.0  461.0  431.0   0.581818   89.406061   0.296296  127.703704  \n",
       "2     58.0  33.0  197.0  127.0  27.194139   92.366300  22.832432   87.870270  \n",
       "3    107.0  10.0  249.0  166.0  52.475096  122.114943   6.632124  110.093264  \n",
       "4      1.0   1.0  154.0  176.0   0.514056   79.164659   0.633663  111.524752  \n",
       "..     ...   ...    ...    ...        ...         ...        ...         ...  \n",
       "231    NaN   NaN    NaN    NaN   0.000000    0.000000   0.000000    0.000000  \n",
       "232    NaN   NaN    NaN    NaN   0.000000    0.000000   0.000000    0.000000  \n",
       "233    NaN   NaN    NaN    NaN   0.000000    0.000000   0.000000    0.000000  \n",
       "234    NaN   NaN    NaN    NaN   0.000000    0.000000   0.000000    0.000000  \n",
       "235    NaN   NaN    NaN    NaN   0.000000    0.000000   0.000000    0.000000  \n",
       "\n",
       "[236 rows x 14 columns]"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "id": "absent-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaccoonDataset(Dataset):\n",
    "    def __init__(self, df, root, augment=True):\n",
    "        # Dataframe with bounding boxes\n",
    "        self.df = df\n",
    "        self.root = root\n",
    "        self.label_dict = {'raccoon':1, 'wild':0}\n",
    "        # define the transformation\n",
    "        # this transformation pipeline will randomly apply vertical flips, horizontal flips\n",
    "        # to images and to bounding boxes \n",
    "        if augment == True:\n",
    "            self.transforms = A.Compose([\n",
    "                A.augmentations.transforms.VerticalFlip(p=.5),\n",
    "                A.augmentations.transforms.HorizontalFlip(p=.5),\n",
    "                ToTensorV2()], bbox_params = A.BboxParams(format='pascal_voc', label_fields=['class_labels'])) \n",
    "        else:\n",
    "            self.transforms = A.Compose([\n",
    "                ToTensorV2()], bbox_params = A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) # this only works if 1:1 image:label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # gathering image information from datafiels \n",
    "        row = self.df.iloc[idx]\n",
    "        fname = row['filename']\n",
    "        img_path = root + '/' + fname\n",
    "        img = cv2.imread(img_path)\n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get bounding box coordinates ()\n",
    "        xmin, ymin = tuple(list(row[['xmin_rs','ymin_rs']].values))\n",
    "        xmax, ymax = tuple(list(row[['xmax_rs','ymax_rs']].values))\n",
    "        \n",
    "        # \"normalizing\" the bounding boxes to the same size as the image\n",
    "        bbox = [[xmin/128,ymin/128,xmax/128,ymax/128]]\n",
    "        class_labels = [row['class']]\n",
    "        \n",
    "        # passing image and bounding box values through the albumentations pipeline \n",
    "        transformed = self.transforms(image=img.astype(np.uint8),bboxes=bbox, class_labels=class_labels)\n",
    "        img = transformed['image'].float()\n",
    "        bbox = torch.tensor(transformed['bboxes'][0])\n",
    "        class_labels = transformed['class_labels']\n",
    "        label = torch.tensor(self.label_dict['raccoon'])\n",
    "        return img, label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "requested-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/michellejanneycoyle/Desktop/Object_Detection_DL/combined_rs'\n",
    "train_ds = RaccoonDataset(train, root)\n",
    "valid_ds = RaccoonDataset(valid, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "ideal-sudan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>xmin_rs</th>\n",
       "      <th>xmax_rs</th>\n",
       "      <th>ymin_rs</th>\n",
       "      <th>ymax_rs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>raccoon-17.jpg</td>\n",
       "      <td>259.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>raccoon</td>\n",
       "      <td>95.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>46.949807</td>\n",
       "      <td>82.532819</td>\n",
       "      <td>39.587629</td>\n",
       "      <td>77.85567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0        filename  width  height    class  xmin  ymin  \\\n",
       "0      0           0  raccoon-17.jpg  259.0   194.0  raccoon  95.0  60.0   \n",
       "\n",
       "    xmax   ymax    xmin_rs    xmax_rs    ymin_rs   ymax_rs  \n",
       "0  167.0  118.0  46.949807  82.532819  39.587629  77.85567  "
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "id": "upset-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "grave-spotlight",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # same padding!\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 100)\n",
    "        self.linear2 = nn.Linear(100, 5)\n",
    "        \n",
    "        # pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # unroll x for FC layer\n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "virtual-picnic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_loss = nn.BCELoss()\n",
    "bbox_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "def train_model(train_dl, valid_dl, num_epochs, model, optimizer, loss_weight):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = 0\n",
    "        for x, y, bbox in train_dl:\n",
    "            model.train()\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            # splitting y_pred into two for respective loss functions\n",
    "            # indexing is different in pytorch\n",
    "            y_pred = torch.split(y_pred, 4, dim=1)\n",
    "            y_pred_bbox = y_pred[0] # Keep it in this form because CrossEntorpyLoss includes Softmax\n",
    "            print(y_pred_bbox)\n",
    "            y_pred_class = y_pred[1]\n",
    "            \n",
    "            loss = class_loss(input=y_pred_class, target=y) + (loss_weight*bbox_loss(input=y_pred_bbox, target=bbox))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.shape[0]\n",
    "            \n",
    "        # Valid\n",
    "        val_loss = 0\n",
    "        num_correct = []\n",
    "        iou_scores = []\n",
    "        for x, y, bbox in valid_dl:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.split(y_pred, 196, dim=1)\n",
    "            y_pred_class = y_pred[0]\n",
    "            y_pred_bbox = y_pred[1]\n",
    "            loss = class_loss(input=y_pred_class, target=y) + (loss_weight*bbox_loss(input=y_pred_bbox, target=bbox))\n",
    "            val_loss += loss.item() * x.shape[0]\n",
    "            y_pred_class = torch.argmax(y_pred_class, dim=1) # Find the index where the max occurs to get the most likely class\n",
    "            correct = y_pred_class.eq(y.data).sum().item()\n",
    "            num_correct.append(correct)\n",
    "\n",
    "        print(f'Epoch {epoch}')\n",
    "        print(f'Train loss: {round(train_loss/len(df_train),4)}  Valid loss: {round(val_loss/len(df_valid),4)}') \n",
    "        print('Valid Accuracy', sum(num_correct)/len(df_valid))\n",
    "#         print('IoU score', round(sum(iou_scores)/len(iou_scores),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "proved-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 91.,   8.,  10.,  ..., 143., 131., 165.],\n",
      "         [ 86.,   0.,  18.,  ...,  97., 121., 139.],\n",
      "         [ 45.,  19.,  23.,  ...,  93., 109., 109.],\n",
      "         ...,\n",
      "         [120.,  98., 114.,  ...,   0.,  66.,  75.],\n",
      "         [136., 126., 116.,  ...,  22.,  13.,  88.],\n",
      "         [119., 128., 119.,  ...,  39.,  63., 141.]],\n",
      "\n",
      "        [[102.,  19.,  19.,  ..., 193., 179., 212.],\n",
      "         [ 97.,   8.,  27.,  ..., 148., 170., 188.],\n",
      "         [ 54.,  28.,  29.,  ..., 146., 161., 160.],\n",
      "         ...,\n",
      "         [164., 142., 159.,  ...,  38., 111., 123.],\n",
      "         [180., 171., 161.,  ...,  63.,  61., 139.],\n",
      "         [163., 173., 164.,  ...,  80., 111., 194.]],\n",
      "\n",
      "        [[ 94.,  11.,  16.,  ..., 144., 139., 176.],\n",
      "         [ 89.,   3.,  24.,  ...,  92., 123., 143.],\n",
      "         [ 49.,  23.,  27.,  ...,  76.,  99., 101.],\n",
      "         ...,\n",
      "         [ 87.,  63.,  76.,  ...,   0.,  56.,  61.],\n",
      "         [101.,  90.,  78.,  ...,  19.,   3.,  73.],\n",
      "         [ 84.,  92.,  81.,  ...,  36.,  51., 126.]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1082-3a69e2ac630f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/virtualenvs/py38_default/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1080-ad5332fd6bbd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/py38_default/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/py38_default/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/py38_default/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [32, 3, 3, 3], but got 3-dimensional input of size [3, 128, 128] instead"
     ]
    }
   ],
   "source": [
    "x, y, bbox = next(iter(train_ds))\n",
    "model = CNN()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "unlikely-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN()\n",
    "# train_model(train_dl, valid_dl, 10, model, optimizer, loss_weight=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-sending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
